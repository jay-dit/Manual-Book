{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e652f47",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets (RDDs)\n",
    "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ff0f9",
   "metadata": {},
   "source": [
    "Основной прикол в том, что RDD используются для вычислениях на целом кластере, об этом стоит думать, когда пишешь код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d400a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicilization\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"appName\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3532d2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-2QU3JH4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>appName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=appName>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c52618fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parallelized Collections\n",
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "type(distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d9d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Datasets\n",
    "# distFile = sc.textFile(\"data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d4935e",
   "metadata": {},
   "source": [
    "PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61eb163",
   "metadata": {},
   "source": [
    "Similarly to text files, SequenceFiles can be saved and loaded by specifying the path. The key and value classes can be specified, but for standard Writables this is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1042580e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))\n",
    "rdd.take(1)\n",
    "# Не работает\n",
    "# rdd.saveAsSequenceFile(\"path/to/file\")\n",
    "# sorted(sc.sequenceFile(\"path/to/file\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789be28d",
   "metadata": {},
   "source": [
    "# Basic Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e32535cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data - ['1 2 3 4 5 6 7 8 9']\n",
      "total length - 17\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"data/numbers.txt\")\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(f'data - {lines.collect()}')\n",
    "print(f'total length - {totalLength}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fedc6ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[46] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineLengths.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058b962",
   "metadata": {},
   "source": [
    "# Passing Functions to Spark\n",
    "Spark’s API relies heavily on passing functions in the driver program to run on the cluster. There are three recommended ways to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f58c73f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myFunc(s):\n",
    "    words = s.split(\" \")\n",
    "    return len(words)\n",
    "\n",
    "sc.textFile(\"data/numbers.txt\").map(myFunc).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f164f",
   "metadata": {},
   "source": [
    "Это просто рандомные примеры, если хотите увидеть все функции, то\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#:~:text=list%20of%20objects.-,Transformations,-The%20following%20table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c42e6d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 2 3 4 5 6 7 8 9']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(\"data/numbers.txt\").distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3fe26",
   "metadata": {},
   "source": [
    "Есть маленькая проблема, я не могу запустить этот код на кластере и посмотреть как оно будет работать\n",
    "\n",
    "По идее всё это отличается от обычного датасета только некой низкоуровневостью и набором реализованных функций, так что если вы работаете с RDD, то просто гуглите в документацию приставку rdd и будет счастье\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18536722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
